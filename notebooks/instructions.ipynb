{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIDENTIAL - DO NOT SHARE\n",
    "\n",
    "# TFX Developer Workshop\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This workshop is designed to introduce TensorFlow Extended (TFX)\n",
    "and help you learn to create your own machine learning\n",
    "pipelines.  We'll follow a typical ML development process,\n",
    "starting by examining the dataset, and end up with a complete\n",
    "working pipeline.  Along the way we'll explore ways to debug\n",
    "and update your pipeline, and measure performance.\n",
    "\n",
    "## Step by Step\n",
    "We'll gradually create our pipelines by working step by step,\n",
    "following a typical ML developer process.  Here are the steps:\n",
    "\n",
    "1. Setup your environment\n",
    "1. Bring up initial pipeline skeleton\n",
    "1. Dive into our data\n",
    "1. Feature engineering\n",
    "1. Training\n",
    "1. Analyzing model performance\n",
    "1. Deployment to production\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Linux\n",
    "* Virtualenv\n",
    "* Python 2.7\n",
    "* Git\n",
    "\n",
    "## Workshop Materials\n",
    "\n",
    "The code is organized by the steps that we're working on, so\n",
    "for each step you'll have the code you need and instructions\n",
    "on what to do with it in the code.py file in the step directory.\n",
    "\n",
    "## What we're doing\n",
    "\n",
    "We’re learning how to create an ML pipeline using TFX\n",
    "\n",
    "* TFX pipelines are appropriate when datasets are large\n",
    "* TFX pipelines are appropriate when training/serving consistency is important\n",
    "* TFX pipelines are appropriate when version management for inference is important\n",
    "* Google uses TFX pipelines for planet-scale ML\n",
    "\n",
    "We’re following a typical ML development process\n",
    "\n",
    "* Understanding our data\n",
    "* Feature engineering\n",
    "* Training\n",
    "* Analyze model performance\n",
    "* Lather, rinse, repeat\n",
    "* Deploy to production\n",
    "\n",
    "## Chicago Taxi Dataset\n",
    "\n",
    "![Taxi](images/taxi.png)\n",
    "\n",
    "![Chicago taxi](images/chicago.png)\n",
    "\n",
    "### Goal - Binary classification\n",
    "\n",
    "Will the customer tip more or less than 20%?\n",
    "\n",
    "## Step 1: Setup your environment\n",
    "\n",
    "In a shell:\n",
    "\n",
    "```bash\n",
    "virtualenv -p python2.7 tfx-workshop\n",
    "source tfx-workshop/bin/activate\n",
    "mkdir tfx; cd tfx\n",
    "\n",
    "# git clone https://github.com/tensorflow/workshops/TFX-DevSummit-2019.git\n",
    "# cp -R <citc>/google3/experimental/users/robertcrowe/TFX-DevSummit-2019 .\n",
    "copy files from GDE shared drive into the 'tfx' directory that you just created.\n",
    "\n",
    "cd TFX-DevSummit-2019/workshop/setup\n",
    "./setup_demo.sh\n",
    "```\n",
    "\n",
    "## Step 2: Bring up initial pipeline skeleton\n",
    "\n",
    "### Hello World\n",
    "\n",
    "In a shell:\n",
    "\n",
    "```bash\n",
    "# Open a new terminal window, and in that window ...\n",
    "source tfx-workshop/bin/activate\n",
    "airflow webserver\n",
    "\n",
    "# Open another new terminal window, and in that window ...\n",
    "source tfx-workshop/bin/activate\n",
    "airflow scheduler\n",
    "\n",
    "# Open a browser and go to http://127.0.0.1:8080\n",
    "# Enable the tfx_example_pipeline_DAG\n",
    "# Trigger the tfx_example_pipeline_DAG\n",
    "\n",
    "# In the Airflow web UI, click on tfx_example_pipeline_DAG\n",
    "# Click on Graph View\n",
    "# Wait for the Setup component to turn dark green (~1 minutes)\n",
    "# Use the refresh button on the right or refresh the page\n",
    "```\n",
    "\n",
    "* Return to DAGs list page in Airflow\n",
    "* Trigger tfx_example_pipeline_DAG\n",
    "* Wait for pipeline to complete\n",
    "  * All dark green\n",
    "  * Use refresh on right side or refresh page\n",
    "\n",
    "![Setup complete](images/step2.png)\n",
    "\n",
    "## Step 3: Dive into our data\n",
    "\n",
    "The first task in any data science or ML project is to understand\n",
    "and clean the data.\n",
    "\n",
    "* Understand the data types for each feature\n",
    "* Look for anomalies and missing values\n",
    "* Understand the distributions for each feature\n",
    "\n",
    "### Components\n",
    "\n",
    "![Data Components](images/examplegen1.png)\n",
    "![Data Components](images/examplegen2.png)\n",
    "\n",
    "ExampleGen\n",
    "\n",
    "* Converts input data to tf.Example\n",
    "\n",
    "StatisticsGen\n",
    "\n",
    "* Uses TensorFlow Data Validation (TFDV) to create descriptive statistics for dataset and features\n",
    "\n",
    "SchemaGen\n",
    "\n",
    "* Uses TensorFlow Data Validation (TFDV) to infer a schema for the dataset\n",
    "\n",
    "ExampleValidator\n",
    "\n",
    "* Uses TensorFlow Data Validation (TFDV) to look for anomalies and missing values\n",
    "\n",
    "### In an editor:\n",
    "\n",
    "```python\n",
    "# Add the following code to ~/airflow/dags/tfx_example_pipeline.py\n",
    "# Add appropriate imports\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "\n",
    "# Add components to pipeline in create_pipeline()\n",
    "statistics_gen = StatisticsGen(\n",
    "  input_data=example_gen.outputs.output)\n",
    "infer_schema = SchemaGen(stats=statistics_gen.outputs.output)\n",
    "validate_stats = ExampleValidator(\n",
    "  stats=statistics_gen.outputs.output,\n",
    "  schema=infer_schema.outputs.output)\n",
    "\n",
    "# Add to return\n",
    "return [example_gen, statistics_gen, infer_schema, validate_stats]\n",
    "```\n",
    "\n",
    "* Return to DAGs list page in Airflow\n",
    "* Trigger tfx_example_pipeline_DAG\n",
    "* Wait for pipeline to complete\n",
    "  * All dark green\n",
    "  * Use refresh on right side or refresh page\n",
    "\n",
    "![Dive into data](images/step3.png)\n",
    "\n",
    "### In a shell:\n",
    "\n",
    "```bash\n",
    "cd <repo>/workshop/notebooks\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### In a browser:\n",
    "\n",
    "* Open step3.ipynb\n",
    "* Follow the notebook\n",
    "\n",
    "![Dive into data](images/step3notebook.png)\n",
    "\n",
    "For a more extensive example of using TFDV to explore and validate a\n",
    "dataset, [see the examples on tensorflow.org](\n",
    "https://www.tensorflow.org/tfx/data_validation)\n",
    "\n",
    "## Step 4: Feature engineering\n",
    "\n",
    "We can increase the predictive quality of our data and/or reduce\n",
    "dimensionality with feature engineering.\n",
    "\n",
    "* Feature crosses\n",
    "* Vocabularies\n",
    "* Embeddings\n",
    "* PCA\n",
    "* Categorical encoding\n",
    "\n",
    "Write Once - The resulting transforms will be consistent between training\n",
    "and serving.\n",
    "\n",
    "### Components\n",
    "\n",
    "![Transform](images/transform.png)\n",
    "\n",
    "Transform\n",
    "\n",
    "* Uses TensorFlow Transform (TFT) to perform data transformations\n",
    "\n",
    "### In a shell:\n",
    "\n",
    "```bash\n",
    "mkdir ~/airflow/plugins/tfx_example\n",
    "cp <repo>/setup/plugins/tfx_example/__init__.py ~/airflow/plugins/tfx_example\n",
    "cp <repo>/setup/plugins/tfx_example/transforms.py ~/airflow/plugins/tfx_example\n",
    "cp <repo>/setup/plugins/tfx_example/features.py ~/airflow/plugins/tfx_example\n",
    "```\n",
    "\n",
    "### In an editor:\n",
    "\n",
    "```python\n",
    "# Add the following code to ~/airflow/dags/tfx_example_pipeline.py\n",
    "# Add appropriate imports\n",
    "from tfx.components import Transform\n",
    "\n",
    "# Add new modules above PipelineDecorator\n",
    "# Modules for trainer and transform\n",
    "plugin_dir = os.path.join(home_dir, 'plugins/tfx_example/')\n",
    "model = os.path.join(plugin_dir, 'model.py')\n",
    "transforms = os.path.join(plugin_dir, 'transforms.py')\n",
    "\n",
    "# Add components to the end of pipeline in create_pipeline()\n",
    "  transform = Transform(\n",
    "      input_data=example_gen.outputs.output,\n",
    "      schema=infer_schema.outputs.output,\n",
    "      module_file=transforms)\n",
    "\n",
    "# Add components to return\n",
    "  return [\n",
    "      example_gen, statistics_gen, infer_schema, validate_stats, transform\n",
    "  ]\n",
    "```\n",
    "\n",
    "* Return to DAGs list page in Airflow\n",
    "* Trigger tfx_example_pipeline_DAG\n",
    "* Wait for pipeline to complete\n",
    "  * All dark green\n",
    "  * Use refresh on right side or refresh page\n",
    "\n",
    "![Feature Engineering](images/step4.png)\n",
    "\n",
    "## Step 5: Training\n",
    "\n",
    "Train a TensorFlow model with our nice, clean, transformed data.\n",
    "\n",
    "* Include the transformations from step 4 so that they are applied\n",
    "consistently\n",
    "* Save the results as a SavedModel for production\n",
    "* Visualize and explore the training process using TensorBoard\n",
    "* Also save an EvalSavedModel for analysis of model performance\n",
    "\n",
    "### Components\n",
    "\n",
    "Trainer\n",
    "\n",
    "* Orchestrates the training of a TensorFlow model\n",
    "\n",
    "### In a shell:\n",
    "\n",
    "```bash\n",
    "cp <repo>/setup/plugins/tfx_example/model.py ~/airflow/plugins/tfx_example\n",
    "```\n",
    "\n",
    "### In an editor:\n",
    "\n",
    "```python\n",
    "# Add the following code to ~/airflow/dags/tfx_example_pipeline.py\n",
    "# Add appropriate imports\n",
    "from tfx.components import Trainer\n",
    "\n",
    "# Add components to the end of pipeline in create_pipeline()\n",
    "  trainer = Trainer(\n",
    "      module_file=model,\n",
    "      transformed_examples=transform.outputs.transformed_examples,\n",
    "      schema=infer_schema.outputs.output,\n",
    "      transform_output=transform.outputs.transform_output,\n",
    "      train_steps=10000,\n",
    "      eval_steps=5000,\n",
    "      warm_starting=True)\n",
    "\n",
    "# Add trainer to return\n",
    "  return [\n",
    "      example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
    "      trainer\n",
    "  ]\n",
    "```\n",
    "\n",
    "* Return to DAGs list page in Airflow\n",
    "* Trigger tfx_example_pipeline_DAG\n",
    "* Wait for pipeline to complete\n",
    "  * All dark green\n",
    "  * Use refresh on right side or refresh page\n",
    "\n",
    "![Training a Model](images/step5.png)\n",
    "\n",
    "### Back on Jupyter\n",
    "\n",
    "* Open step5.ipynb\n",
    "* Follow the notebook\n",
    "\n",
    "![Training a Model](images/step5tboard.png)\n",
    "\n",
    "## Step 6: Analyzing model performance\n",
    "\n",
    "Understanding more than just the top level metrics.\n",
    "\n",
    "* Users experience model performance for their queries only\n",
    "* Poor performance on slices of data can be hidden by top level\n",
    "metrics\n",
    "* Model fairness is important\n",
    "* Often key subsets of users or data are very important, and may\n",
    "be small\n",
    "    * Performance in critical but unusual conditions\n",
    "    * Performance for key audiences such as influencers\n",
    "\n",
    "### Components\n",
    "\n",
    "Evaluator\n",
    "\n",
    "* Uses TensorFlow Model Analysis to perform deep analysis of the performance of the model that we trained\n",
    "\n",
    "### In an editor:\n",
    "\n",
    "```python\n",
    "# Add the following code to ~/airflow/dags/tfx_example_pipeline.py\n",
    "# Add appropriate imports\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import Evaluator\n",
    "\n",
    "# Add components to the end of pipeline in create_pipeline()\n",
    "  model_analyzer = Evaluator(\n",
    "      examples=example_gen.outputs.output,\n",
    "      model_exports=trainer.outputs.output)\n",
    "\n",
    "# Add model_analyzer to return\n",
    "  return [\n",
    "      example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
    "      trainer, model_analyzer\n",
    "  ]\n",
    "```\n",
    "\n",
    "* Return to DAGs list page in Airflow\n",
    "* Trigger tfx_example_pipeline_DAG\n",
    "* Wait for pipeline to complete\n",
    "  * All dark green\n",
    "  * Use refresh on right side or refresh page\n",
    "\n",
    "![Analyzing model performance](images/step6.png)\n",
    "\n",
    "### Back on Jupyter:\n",
    "\n",
    "* Open step6.ipynb\n",
    "* Follow the notebook\n",
    "\n",
    "![Analyzing model performance](images/step6notebook.png)\n",
    "\n",
    "For a more extensive example of using TFMA to analyze model\n",
    "performance, [see the examples on tensorflow.org](\n",
    "https://www.tensorflow.org/tfx/model_analysis)\n",
    "\n",
    "## Step 7: Deployment to production\n",
    "\n",
    "If the new model is ready, make it so.\n",
    "\n",
    "* If we’re replacing a model that is currently in production, first\n",
    "make sure that the new one is better\n",
    "* ModelValidator tells Pusher if the model is OK\n",
    "* Pusher deploys SavedModels to well-known locations\n",
    "\n",
    "Deployment targets receive new models from well-known locations\n",
    "\n",
    "* TensorFlow Serving\n",
    "* TensorFlow Lite\n",
    "* TensorFlow JS\n",
    "* TensorFlow Hub\n",
    "\n",
    "### Components\n",
    "\n",
    "ModelValidator\n",
    "\n",
    "* Compares multiple versions of the trained model to make sure that the new version meets requirements\n",
    "\n",
    "Pusher\n",
    "\n",
    "* If the model passes ModelValidator, Pusher deploys the SavedModel to a well-known location\n",
    "\n",
    "### In an editor:\n",
    "\n",
    "```python\n",
    "# Add the following code to ~/airflow/dags/tfx_example_pipeline.py\n",
    "# Add appropriate imports\n",
    "from tfx.components import ModelValidator\n",
    "from tfx.components import Pusher\n",
    "\n",
    "# Add components to the end of pipeline in create_pipeline()\n",
    "  model_validator = ModelValidator(\n",
    "      examples=example_gen.outputs.output,\n",
    "      model=trainer.outputs.output)\n",
    "\n",
    "  pusher = Pusher(\n",
    "      model_export=trainer.outputs.output,\n",
    "      model_blessing=model_validator.outputs.blessing,\n",
    "      serving_model_dir=serving_model_dir)\n",
    "\n",
    "# Add model_analyzer to return\n",
    "  return [\n",
    "      example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
    "      trainer, model_analyzer, model_validator, pusher\n",
    "  ]\n",
    "```\n",
    "\n",
    "* Return to DAGs list page in Airflow\n",
    "* Trigger tfx_example_pipeline_DAG\n",
    "* Wait for pipeline to complete\n",
    "  * All dark green\n",
    "  * Use refresh on right side or refresh page\n",
    "\n",
    "![Deployment to production](images/step7.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
